## Tasks Completed

- Presented my research introduction to the group
- Able to download the CUDA tool `nvvp` (Nvidia Visual Profiler) and explore the data points that it collected
  - ![Visual Profiler](pics/visual_profiler_ex1.png) 

## Notes

- Writing a pipeline in bash to execute the docker command to run nvprof and then the object detector. Nvprof attaches to the Python process and then writes a log file that can be inspected with Nvidia Visual Profiler.

  - Currently trying to open the log file with nvvp (the visual profiler)
  - The visual profiler provides some helpful insights into the app's performance like "low memcpy throughput" and "low kernel concurrency" which might help assist researchers to build better apps
  - It also lists a timeline of GPU-Host actions through CUDA
- How does Linux treat memory? (And specifically how does Docker treat memory?) - *virtual memory* is the working memory of processes on Linux. It has a number of advantages being separate from a physical address on RAM: large address spaces, protected process spaces, memory mapping, fair memory allocation, and memory shares.
  - Sources: https://www.golinuxcloud.com/tutorial-linux-memory-management-overview/, 
  - **Inter Process Communication** might be an interesting research topic too
    - It would be a relevant topic to help make resource-limited IoT apps more efficient. For example, it would make sense to share the same libraries among different app containers, especially if all of them do similar tasks. (Although I'm sure Docker already shares the same host libraries among containers) 
- Literature review: https://dl.acm.org/doi/abs/10.1145/2820975.2820980?casa_token=qZXife_dq3sAAAAA%3ACsIpOcrQbZK95dsTU8kZ_k1gRxFpPdVqjXESNVTNxu_RsSmObS5HjX-H4xHndKhjXfMxosHl5wjqyQ, "An Early Resource Characterization of Deep Learning on Wearables, Smartphones and Internet-of-Things Devices"
- Now that I have two tools that can collect the CPU, GPU, and memory usage of the board as it runs, I will start progress on a script that can automate the collection of these data points over time
  - `nvprof` does not support gathering the metric of max GPU memory usage (according to here: https://forums.developer.nvidia.com/t/how-to-get-the-maximum-gpu-memory-usage-with-nvprof/117373)
    - It appears you can track the DRAM throughput of a specific kernel but not the whole process? (Following this presentation on iteratively optimizing a cuda program with nvprof and system analyzer: https://calcul.math.cnrs.fr/attachments/spip/IMG/pdf/CUDA-Optimization-Julien-Demouth.pdf)

