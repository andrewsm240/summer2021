## Tasks Completed

- 

## Notes

- Watching presentation video on making containers from scratch (https://www.youtube.com/watch?v=8fi7uSYlOdc)
  - In Linux there are different namespaces which allow system resources to be shared among processes and allow for aliasing (there can be a hostname namespace in a spawned process that is different than the main host's hostname) 
    - Mount namespace - mount points are copied to the new namespace and then from there on new mounts can be created in that namespace that are separate from the host's
    - Process ID namespace - creates a new process ID list with the original process being id 1
    - `cgroups` - control groups that control how many resources the processes are getting, i.e. number of processes able to be started in the container
- Docker research and running multiple ML models
  - Storage driver
    - The default storage driver is `overlay2` which mounts filesystems at `/var/lib/docker/overlay2/` which can be accessed through simple Linux filesystem commands
    - The command `mount | grep overlay` displays the container that is currently in use
  - Running multiple ML models
    - To best run 2 ML models, use TensorRT which is optimized for the platform: https://forums.developer.nvidia.com/t/running-two-neural-networks-simultaneously/180677
    - TensorRT is the framework that deploys models on DLA (the NX's deep learning accelerators)
    - The NVDLA Project appears to be a platform-independent deep learning accelerator system: http://nvdla.org/primer.html which may be relevant to the NX
  - Was able to run Yongho's code: `docker run -d --name objcounter -v /tmp/data-config.json:/run/waggle/data-config.json --runtime nvidia --network host waggle/plugin-objectcounter:0.0.0 -stream bottom_image -object person -interval 1` 
    -  `docker stats`  can show some simple memory/cpu stats
    - My program can output GPU usage every 1s (although in the future I may want to edit its hardcoded tegrastats call so that it can update faster)
    - I will need to check out the utilities in the cuda-10 bin. For example `nvprof` sounds like it could be useful, but with docker it says that no application was profiled.

